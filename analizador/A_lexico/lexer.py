import ply.lex as lex

# Definici贸n de los nombres de los tokens que el lexer reconocer谩
tokens = (
    'NODO', 'ELIMINAR_NODO', 'ARISTA', 'ELIMINAR_ARISTA', 'PESO', 'RUTA_MINIMA', 'NO_D',
    'ID', 'NUMBER', 'LPAREN', 'RPAREN', 'COMMA', 'SEMICOLON'
)

# Diccionario de palabras reservadas y sus tokens correspondientes
reserved = {
    'nodo': 'NODO',
    'eliminar_nodo': 'ELIMINAR_NODO',
    'arista': 'ARISTA',
    'eliminar_arista': 'ELIMINAR_ARISTA',
    'peso': 'PESO',
    'ruta_minima': 'RUTA_MINIMA',
    'no_d': 'NO_D'
}

# Definiciones de tokens simples usando expresiones regulares
t_LPAREN    = r'\('
t_RPAREN    = r'\)'
t_COMMA     = r','
t_SEMICOLON = r';'

# Funci贸n para reconocer identificadores y palabras reservadas
def t_ID(t):
    r'[A-Za-z][A-Za-z0-9_]*'
    t.type = reserved.get(t.value.lower(), 'ID')  
    return t

# Funci贸n para reconocer n煤meros
def t_NUMBER(t):
    r'\d+'
    t.value = int(t.value)  
    return t

# Funci贸n para manejar nuevas l铆neas
def t_newline(t):
    r'\n+'
    t.lexer.lineno += len(t.value)
    print(f"Actualizando n煤mero de l铆nea a {t.lexer.lineno}")

# Ignorar espacios y tabulaciones
t_ignore = ' \t'

# Comentarios de una l铆nea
def t_COMMENT(t):
    r'//.*'  
    pass

# Comentarios multil铆nea
def t_COMMENT_BLOCK(t):
    r'/\*.*?\*/'  
    pass

# Funci贸n para manejar errores l茅xicos
def t_error(t):
    raise Exception(f"Error l茅xico en la posici贸n {t.lexpos}, l铆nea {t.lineno}")
    # print(f" Error l茅xico: Car谩cter inv谩lido '{t.value[0]}' en la posici贸n {t.lexpos}")
    # t.lexer.skip(1)

# Crear una instancia del lexer
lexer = lex.lex()

# Funci贸n para analizar el c贸digo de entrada y devolver una lista de tokens
def analizar_codigo(data):
    lexer.input(data)
    tokens_list = []
    while True:
        tok = lexer.token()
        if not tok:
            break  
        tokens_list.append(tok)
        
    return tokens_list

# C贸digo principal para probar el lexer
if __name__ == "__main__":
    codigo_prueba = "nodo(A);\narista(A, B);"
    print("\n An谩lisis L茅xico:")
    tokens = analizar_codigo(codigo_prueba)
    print("\n Lista de Tokens:")
    for token in tokens:
        print(f"Tipo: {token.type}, Valor: {token.value}, L铆nea: {token.lineno}, Posici贸n: {token.lexpos}")
