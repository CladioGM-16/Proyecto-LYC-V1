import ply.lex as lex

# Definici칩n de los nombres de los tokens que el lexer reconocer치
tokens = (
    'NODO', 'ELIMINAR_NODO', 'ARISTA', 'ELIMINAR_ARISTA', 'PESO', 'RUTA_MINIMA', 'NO_D',
    'ID', 'NUMBER', 'LPAREN', 'RPAREN', 'COMMA', 'SEMICOLON'
)

# Diccionario de palabras reservadas y sus tokens correspondientes
reserved = {
    'nodo': 'NODO',
    'eliminar_nodo': 'ELIMINAR_NODO',
    'arista': 'ARISTA',
    'eliminar_arista': 'ELIMINAR_ARISTA',
    'peso': 'PESO',
    'ruta_minima': 'RUTA_MINIMA',
    'no_d': 'NO_D'
}

# Definiciones de tokens simples usando expresiones regulares
t_LPAREN    = r'\('
t_RPAREN    = r'\)'
t_COMMA     = r','
t_SEMICOLON = r';'

# Funci칩n para reconocer identificadores y palabras reservadas
def t_ID(t):
    r'[A-Za-z][A-Za-z0-9_]*'
    t.type = reserved.get(t.value.lower(), 'ID')  
    return t

# Funci칩n para reconocer n칰meros
def t_NUMBER(t):
    r'\d+'  
    t.value = int(t.value)  
    return t

# Funci칩n para manejar nuevas l칤neas
def t_newline(t):
    r'\n+'
    t.lexer.lineno += len(t.value)

# Ignorar espacios y tabulaciones
t_ignore = ' \t'

# Comentarios de una l칤nea
def t_COMMENT(t):
    r'//.*'  
    pass

# Comentarios multil칤nea
def t_COMMENT_BLOCK(t):
    r'/\*.*?\*/'  
    pass

# Funci칩n para manejar errores l칠xicos
def t_error(t):
    print(f"游띔 Error l칠xico en la l칤nea {t.lineno}, posici칩n {t.lexpos}:")
    print(f"  Car치cter inv치lido: '{t.value[0]}'")
    
    line_start = max(t.lexer.lexdata.rfind('\n', 0, t.lexpos) + 1, 0)  
    line_end = t.lexer.lexdata.find('\n', t.lexpos)
    if line_end == -1: line_end = len(t.lexer.lexdata)  
    line = t.lexer.lexdata[line_start:line_end]  
    
    # Mostrar la l칤nea donde ocurri칩 el error
    print(f"  L칤nea del error: {line}")
    print(f"  {' ' * (t.lexpos - line_start+ 17)}^")    

    # Marcar error_ocurrido como True
    t.lexer.error_ocurrido = True  # Establecer bandera de error

    # No lanzar excepci칩n, solo continuar con el siguiente car치cter
    t.lexer.skip(1)  # Ignorar el car치cter inv치lido y continuar el an치lisis


# Crear una instancia del lexer
lexer = lex.lex()

# Funci칩n para analizar el c칩digo de entrada y devolver una lista de tokens
def analizar_codigo(data):
    if not data:  # Verificar si la entrada est치 vac칤a
        print("游뛂 La entrada est치 vac칤a. No se encontraron tokens.")
        return []  # Regresar una lista vac칤a

    lexer.input(data)
    lexer.error_ocurrido = False  # Inicializar bandera de error
    tokens_list = []
    
    # Procesar cada token en el c칩digo
    while True:
        tok = lexer.token()
        if not tok:
            break  # No m치s tokens
        tokens_list.append(tok)

    # Mostrar mensaje de 칠xito o error solo al final del proceso
    if lexer.error_ocurrido:
        print("丘멆잺 El an치lisis l칠xico termin칩 con errores.")
    else:
        print("九덢잺 El an치lisis l칠xico se complet칩 correctamente.")
    
    return tokens_list

# C칩digo principal para probar el lexer
if __name__ == "__main__":
    # Ejemplo de c칩digo v치lido
    print("\n游늷 An치lisis L칠xico:")
    codigo_prueba = "nodo(a)\nnodo(@)"
    tokens = analizar_codigo(codigo_prueba)
    print("\n游늶 Lista de Tokens:")
    for token in tokens:
        print(f"Tipo: {token.type}, Valor: {token.value}, L칤nea: {token.lineno}, Posici칩n: {token.lexpos}")
